---
title: "Seq+Random Holdout - Models"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    df_print: paged
  pdf_document: default
fontsize: 12pt
header-includes: 
- \usepackage{longtable}
classoption: landscape, a4paper
---

## Setup
```{r setup, include=FALSE}
options(digits = 3)
library(knitr)
library(haven)
library(kknn)
library(dplyr)
library(magrittr)
library(kableExtra)
library(tidyr)
library(tidyverse)
library(readxl)
library(rpart)
library(rpart.plot)
library(party)
library(car)
library(randomForest)
library(stargazer)
library(lme4)
library(lmerTest)
library(lme4)
library(caret)
library(GGally)
library(reshape2)
library(grid)
library(gridExtra)
library(dummies)
library(corrplot)
library(magrittr)
library(DT)
library(xtable)
library(kableExtra)
library(jtools)
library(reshape2)
library(boot)
library(pastecs)
library(fpp)
library(forecast)
library(data.table)
library(ISLR)
library(e1071)
library(FNN) 
library(neuralnet)
library(nnet)
library(purrr)
knitr::opts_chunk$set(error=TRUE,fig.width=5, fig.height=5)
```

```{r errfunc, include=FALSE}
#Function to calculate errors 
errors = function(data, true, pred) {
  require(dplyr)
  true <- enquo(true)
  pred <- enquo(pred)
  data = data %>%
    mutate(., Error = !! pred - !! true) %>%
    mutate(., Abs_Err = abs(!! pred - !! true)) %>%
#    mutate(., Percent_Err = (!! pred - !! true)/(!! true)) %>%
#    mutate(., SE = (!! pred - !! true)^2) %>%
    mutate(., APE = abs((!! pred - !! true)/(!! true)))
 return (data)
}
```

```{r dataset, include=FALSE}
arr <- read.csv("~/Documents/Research/Thesis writing/NewData/Arr.csv") %>%
  data.frame()%>%
  select(Arrival.Date, Booking.Window, Quantity) %>%
  mutate_at(., vars(Arrival.Date),  funs(as.Date(., "%m/%d/%Y")))
```

```{r, echo=FALSE}
dt = arr %>%
  group_by(Arrival.Date, Booking.Window) %>%
  mutate(Quan = sum(Quantity)) %>%
  dplyr::arrange(., Arrival.Date, Booking.Window) 

dt = dt[-(1:2),-3] #throw out the first two observations to avoid inconsistency

foo = function(x){
  require(dplyr)
  new = as.Date(x, '%y.%m.%d') %>%
     format(., '20%y-%m-%d') %>%
    as.Date()
  return (new)
}

dt$Arrival.Date = foo(dt$Arrival.Date)
```

The independent variables used in this sections are: 
1) ROH on each DBA; 
2) Day of week of the arrival day;

```{r wide, message=FALSE, echo = FALSE}
#accumulated ROH_DBA
wide = dcast(dt, Arrival.Date ~ Booking.Window, value.var='Quan') %>% 
  data.frame() %>%
  arrange(., Arrival.Date)

for (i in (ncol(wide)-1):2){
  wide[i] = wide[i] + wide[i+1] 
}
wide[,3] = as.factor(weekdays(wide$Arrival.Date))
colnames(wide) = c('Arrival.Date', 'True', 'DOW', paste0('ROH_D', 1:(ncol(wide)-3)))

wide$DOW <- ordered(wide$DOW, levels=c("Sunday","Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))

wide = wide %>%
  remove_rownames %>% column_to_rownames('Arrival.Date')

kable(wide[1:10, 1:10], caption = 'Data Preview') %>%
  kable_styling(latex_options = c("striped", "hold_position")) 
```

# 2 Random cross validation
## 2.1 Advanced Booking Models
```{r matrix.error.function, include = FALSE} 
materr = function(data){
  #requirements for data: 
  #first column is true value, 
  #the rest of the columns records the predictions of different parameters
  require(dplyr)
  #generating error containers
  m = data 
  for (j in 2:ncol(data)){
    m[,j] = (data[,j] - data[,1])  #calculating errors for each forecast
  }
  me = colMeans(m)
  mse = colMeans(m^2)
  mae = colMeans(abs(m))
  #print(abs(m))
  #print(m)
  
  p = data
  for (i in 2:ncol(p)){
    p[, i] = m[,i] / data[,1]
  }
  #print(p)
  mpe = colMeans(p)
  mape = colMeans(abs(p))

  return(rbind(me, mse, mae, mpe, mape))
}
```

```{r test.materr.function, include = FALSE, eval=FALSE}
#dont delete this!
tt= rnorm(200, 10, 5) %>% matrix(., ncol = 10) %>% as.data.frame()
tt[,1] = rnorm(nrow(tt), 100, 20)
tt[2,2] = 160
materr(tt)
```

```{r Ransampling}
tr_ind = sample(nrow(wide), 0.8*nrow(wide))
train = wide[tr_ind, ]
test = wide[-tr_ind,]
```

### 2.1.1 Additive Advanced Booking Models

```{r ran.apk} 
#calculating pickup
ct1 = train[, -2, drop=FALSE]
for (i in ncol(ct1):1){
  ct1[,i] = ct1[,1]- ct1[,i]
}
ran.apk.avg = data.frame(colMeans(ct1))
ran.apk.plot = plot(ran.apk.avg[,1], cex=.5,
     main = 'Additive Pickup', xlab = "Days before arrival", ylab='average pickup')
```

```{r ran.apk.pre}
ran.apk.pre = test[, -2] 
ran.apk.pre[, 2:ncol(ran.apk.pre)] = NA

for (i in 1:(nrow(ran.apk.pre))){
  ran.apk.pre[i, ] = as.data.frame(t(ran.apk.avg)) + test[i, -2]
}

ran.apk.pre[1:10, 1:10]%>%
  kable(., 'latex', caption = 'Additive Pickup Predictions', 
        longtable = T, booktabs = T) %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"))
```

```{r, include = FALSE, eval=FALSE}
par(mfrow = c(3,2))
days = sort(sample(1:nrow(ran.apk.pre), 10, replace=FALSE))
for (i in days){
  y = ran.apk.pre[i, ]
  h = ran.apk.pre[i, 1]
  plot(x = 1:ncol(ran.apk.pre), y = ran.apk.pre[i, ], main = paste('Day',i), xlab='', ylab='')
  abline(h = ran.apk.pre[i,1], col='red')
}
```

```{r ran.apk.err.plot, echo=TRUE, message=FALSE}
ran.apk.err = materr(ran.apk.pre) %>% as.data.frame()
ran.apk.plot = melt(setDT(ran.apk.err, keep.rownames = TRUE), 'rn')

par(mfrow = c(1,2))
ggplot(aes(x=variable, y=value, group=rn, color = rn), 
       data = subset(ran.apk.plot , rn %in% c('mae'))[-1,]) +
  geom_line() +
  ggtitle('Mean Absolute Error for Adv.Pickup') +
  xlab('DBA') + ylab('MAE')

ggplot(aes(x=variable, y=value, group=rn, color = rn), 
       data = subset(ran.apk.plot , rn %in% c('mape'))[-1,]) +
  geom_line() +
  ggtitle('Mean Absolute Percentage Error for Adv.Pickup') +
  xlab('DBA') + ylab('MAPE')
```

### 2.1.2 Additive Advance Booking (with weekly differentiation)

```{r r.apk.dow, echo=TRUE}
ran.apk.wd = train
for (j in ncol(ran.apk.wd):3){
  ran.apk.wd[,j] = ran.apk.wd[,1] - ran.apk.wd[,j]
}

ran.apk.wd = ran.apk.wd %>%
  group_by(DOW) %>%
  summarise_at(.vars=names(.)[3:ncol(ran.apk.wd)], .funs='mean')

justplot.rapkwd = melt(ran.apk.wd, id='DOW')
ggplot(justplot.rapkwd, 
       aes(x=variable, y=value, group=DOW, color = DOW)) +
  geom_line() +
  scale_color_brewer(palette="Accent")+
  ggtitle('Additive Pickups (by weekday, ran HO)') +
  xlab('DBA') + ylab('pickups')
```

```{r r.apk.wd.pred, echo=FALSE}
ran.apk.wd.pre = test
ran.apk.wd.pre[, 3:ncol(ran.apk.wd.pre)] = NA

for (i in 1:(nrow(ran.apk.wd.pre))){
  m = match(ran.apk.wd.pre[i,2], ran.apk.wd$DOW)
  ran.apk.wd.pre[i,3:ncol(ran.apk.wd.pre)] = 
    test[i,3:ncol(ran.apk.wd.pre)] + ran.apk.wd[m, 2:ncol(ran.apk.wd)]
}

ran.apk.wd.pre[1:10, 1:10]%>%
  kable(., 'latex', caption = 'Additive Pickup Predictions', 
        longtable = T, booktabs = T) %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"))
```

```{r ran.apkwd.err.plot, echo=TRUE, message=FALSE}
ran.apk.wd.err = materr(ran.apk.wd.pre[,-2]) %>% as.data.frame()
ran.apk.wd.plot = melt(setDT(ran.apk.wd.err, keep.rownames = TRUE), 'rn')

par(mfrow = c(1,2))
ggplot(aes(x=variable, y=value, group=rn, color = rn), 
       data = subset(ran.apk.wd.plot , rn %in% c('mae'))[-1,]) +
  geom_line() +
  ggtitle('Mean Absolute Error for Adv.Pickup (weekday diff)') +
  xlab('DBA') + ylab('MAE')

ggplot(aes(x=variable, y=value, group=rn, color = rn), 
       data = subset(ran.apk.plot , rn %in% c('mape'))[-1,]) +
  geom_line() +
  ggtitle('Mean Absolute Percentage Error for Adv.Pickup (weekday diff)') +
  xlab('DBA') + ylab('MAPE')
```


### 2.1.3 Multiplicative Advance Booking
```{r ran.mpk} 
ran.mpk.avg = colMeans(train[,-2]) %>% data.frame() %>% t()
for (i in ncol(ran.mpk.avg):1){
  ran.mpk.avg[1,i] = ran.mpk.avg[1,i] / ran.mpk.avg[1,1] 
}

plot(t(ran.mpk.avg), cex=.5,
     main = 'Multiplicative Pickup Ratio', xlab = "Days before arrival", ylab='average pickup')
```

```{r ran.mpk.pre}
ran.mpk.pre = test[, -2] 
ran.mpk.pre[, 2:ncol(ran.mpk.pre)] = NA

for (i in 1:(nrow(ran.mpk.pre))){
  ran.mpk.pre[i, ] = as.data.frame(ran.mpk.avg) + test[i, -2]
}

ran.mpk.pre[1:10, 1:10]%>%
  kable(., 'latex', caption = 'Multiplicative Pickup Predictions', 
        longtable = T, booktabs = T) %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"))
```

```{r ran.mpk.err.plot, echo=TRUE, message=FALSE}
ran.mpk.err = materr(ran.mpk.pre) %>% as.data.frame()
ran.mpk.plot = melt(setDT(ran.mpk.err, keep.rownames = TRUE), 'rn')

par(mfrow = c(1,2))
ggplot(aes(x=variable, y=value, group=rn, color = rn), 
       data = subset(ran.mpk.plot , rn %in% c('mae'))[-1,]) +
  geom_line() +
  ggtitle('Mean Absolute Error for Mult.Pickup') +
  xlab('DBA') + ylab('MAE')

ggplot(aes(x=variable, y=value, group=rn, color = rn), 
       data = subset(ran.mpk.plot , rn %in% c('mape'))[-1,]) +
  geom_line() +
  ggtitle('Mean Absolute Percentage Error for Multi.Pickup') +
  xlab('DBA') + ylab('MAPE')
```



### 2.1.4 Multiplicative Advance Booking (with weekly differentiation)

```{r ran.mpk.dow} 
ran.mpk.wd = train
for (j in ncol(ran.mpk.wd):3){
  ran.mpk.wd[,j] =  ran.mpk.wd[,j]  / ran.mpk.wd[,1]
}
ran.mpk.wd = ran.mpk.wd %>%
  group_by(DOW) %>%
  summarise_at(.vars=names(.)[3:(ncol(train))], .funs='mean')

justplot.mpkwd = melt(mpk.wd, id='DOW')
ran.mpk.wd[, 1:10] %>%
  kable(., 'latex', caption = 'Mupti Pickup Ratios (by weekday)', 
        longtable = T, booktabs = T) %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"))
#ggplot(justplot.mpkwd, 
#       aes(x=variable, y=value, group=DOW, color = DOW)) +
#  geom_line() +
#  scale_color_brewer(palette="Accent")+
#  ggtitle('Multi Pickup Ratios (by weekday)') +
#  xlab('DBA') + ylab('pickups')
```

```{r ran.mpk.wd.pred} 
ran.mpk.wd.pre = test
ran.mpk.wd.pre[, 3:ncol(ran.mpk.wd.pre)] = NA

for (i in 1:(nrow(ran.mpk.wd.pre))){
  m = match(ran.mpk.wd.pre[i,2], ran.mpk.wd$DOW)
  ran.mpk.wd.pre[i, 3:ncol(ran.mpk.wd.pre)] = 
    test[i, 3:ncol(test)]/
    ran.mpk.wd[m, 2:ncol(ran.mpk.wd)]
}
```

```{r plot.multi.wd.prediction}
plot(x= 1:(ncol(ran.mpk.wd.pre)-1), y= ran.mpk.wd.pre[30, -2],
     main = paste0('Prediction for ', rownames(ran.mpk.wd.pre[30, -2])),
     xlab = 'DBA', ylab = 'Predictions')
abline(h = ran.mpk.wd.pre[30, 1], col = 'red')
legend(1, 100, legend=c("Predictions", "True"),
       col=c("black", "red"), lty=1:2, cex=0.8,  box.lty=0)
```

```{r mpk.wd.err.plot, echo=TRUE, message=FALSE} 
ran.mpk.wd.err = materr(ran.mpk.wd.pre[,-2]) %>% as.data.frame()
ran.mpk.wd.plot = melt(setDT(ran.mpk.wd.err, keep.rownames = TRUE), 'rn')

par(mfrow = c(1,2))
ggplot(aes(x=variable, y=value, group=rn, color = rn), 
       data = subset(ran.mpk.wd.plot , rn %in% c('mae'))[-1,]) +
  geom_line() +
  ggtitle('Mean Absolute Error for Mult.Pickup') +
  xlab('DBA') + ylab('MAE')

ggplot(aes(x=variable, y=value, group=rn, color = rn), 
       data = subset(ran.mpk.wd.plot , rn %in% c('mape'))[-1,]) +
  geom_line() +
  ggtitle('Mean Absolute Percentage Error for Multi.Pickup') +
  xlab('DBA') + ylab('MAPE')
```

## 2.2 Regression
```{r ran.reg.pre, warning=FALSE}
ran.reg.pred = test
ran.reg.pred[,3:ncol(ran.reg.pred)] = NA

for (i in 1:(ncol(ran.reg.pred)-2)){
  this.predictor = paste0('ROH_D',i)
  lm.formula = paste('True', this.predictor, sep = '~')
  reg = lm(lm.formula, data = train)
  for (j in 1:(nrow(ran.reg.pred))){
    ran.reg.pred[j,(i+2)] = predict(reg, test[j,(i+2), drop = FALSE])
  }
}
```

```{r ran.reg.pred.plot}
plot(x= 1:(ncol(ran.reg.pred)-1), y= ran.reg.pred[20, -2],
     main = paste0('Prediction for ', rownames(ran.reg.pred[20, -2])),
     xlab = 'DBA', ylab = 'Predictions')
abline(h = ran.reg.pred[20, 1], col = 'red')
legend(1, 100, legend=c("Predictions", "True"),
       col=c("black", "red"), lty=1:2, cex=0.8,  box.lty=0)
```

```{r ran.reg.err}
ran.reg.err = materr(ran.reg.pred[,-2])
```

```{r ran.mpk.wd.err.plot, echo=TRUE, message=FALSE}
ran.reg.err = materr(ran.reg.pred[,-2]) %>% as.data.frame()
ran.reg.plot = melt(setDT(ran.reg.err, keep.rownames = TRUE), 'rn')

par(mfrow = c(1,2))
ggplot(aes(x=variable, y=value, group=rn, color = rn), 
       data = subset(ran.reg.plot , rn %in% c('mae'))[-1,]) +
  geom_line() +
  ggtitle('Mean Absolute Error for Regression') +
  xlab('DBA') + ylab('MAE')

ggplot(aes(x=variable, y=value, group=rn, color = rn), 
       data = subset(ran.reg.plot , rn %in% c('mape'))[-1,]) +
  geom_line() +
  ggtitle('Mean Absolute Percentage Error for Regression') +
  xlab('DBA') + ylab('MAPE')
```


## 2.3 K-NN
```{r ran.knn.preProcess, messages=FALSE, warning=FALSE}
#Convert categorical variable into dummy variables
train.true = train[,1] 
dow.tr = dummy(train$DOW, sep='.')
dow.te = dummy(test$DOW, sep='.')
k.train = cbind(train[, -c(2, nearZeroVar(train))], dow.tr) #no weekday
k.test = cbind(test, dow.te)[,-2]
```

### 2.3.1 KNN 

* Repeted cross-validation with 10 folds, repeating 3 times
* Each time randomly test 8 k-value, then choose the optimal 
* applied PCA with a threshold of 80%

So for this knn:
train: all training obs, ncol # of models
test: using [, 1:ncol], [,2:ncol], ... to generate predictions

```{r ran.knn.pre, cache=TRUE, messages=FALSE, error=TRUE} 
ran.knn.pre = test[,-2]
ran.knn.pre[, 2:ncol(ran.knn.pre)] = NA

opk1 = rep(NA, nrow(test))
for (i in 2:(ncol(k.train)-2)){
  #each loop takes 1min
  this.train = k.train[, c(1, i:ncol(k.train))]
  train.control = trainControl(method='repeatedcv', number=10, repeats = 3, preProcOptions = list(thresh=0.8)) #for PCA
  k = train(True~., method='knn', tuneLength = 8, 
         #tuneGrid = expand.grid(k = 5:15),
            trControl=train.control, preProcess=c('scale','center','pca'),
            data=this.train)
  opk1[i-1] = k$bestTune[[1]]
  this.test = k.test[, c(i:ncol(k.test))]
  ran.knn.pre[,i] = predict(k, this.test)
}
```

```{r ran.knn.err, cache=TRUE, messages=FALSE}
ran.knn.err = materr(ran.knn.pre)
```
 
```{r ran.knn.err.plot} 
ran.knn.plot = melt(setDT(as.data.frame(ran.knn.err), keep.rownames = TRUE), 'rn')

ggplot(aes(x=variable, y=value, group=rn, color = rn), 
       data = subset(ran.knn.plot , rn %in% c('mae'))[-1,]) +
  geom_line() +
  ggtitle('Mean Absolute Error for knn') +
  xlab('DBA') + ylab('MAE')

ggplot(aes(x=variable, y=value, group=rn, color = rn), 
       data = subset(ran.knn.plot , rn %in% c('mape'))[-1,]) +
  geom_line() +
  ggtitle('Mean Absolute Percentage Error for knn') +
  xlab('DBA') + ylab('MAPE')
```


### 2.3.2 Weighted KNN model 

 * Weighted KNN: it uses kernel functions to weight the neighbors according to their distances. Each time we cross-validation find the optimal kernel. 
 * Euclidean distance

```{r ran.wknn.pred, cache=TRUE, messages=FALSE}
ran.wknn.pre = test[,-2]
ran.wknn.pre[, 2:ncol(ran.wknn.pre)] = NA

opk2 = rep(NA, ncol(test))
for (i in 2:(ncol(k.train)-1)){
  # 2s per loop
  this.train = k.train[, c(1, i:ncol(k.train))]
  train.control = trainControl(method='repeatedcv', number=10, repeats = 3, preProcOptions = list(thresh=0.8)) #for PCA
  wk = train.kknn(True~., data = this.train, kmax = 20, 
                  kernel =  c("rectangular", "triangular", "epanechnikov", "gaussian", "rank", "optimal"))
  opk2[i-1] = wk$best.parameters[[2]]
  
  this.test = k.test[, c(1, i:ncol(k.test))]
  kknn <- kknn(True~., k=opk2[i-1], scale = TRUE, distance=2, 
               train = this.train, test = this.test,
               kernel = paste0(toString(wk$best.parameters[[1]])))
  ran.wknn.pre[,i] = predict(wk, this.test)
}
```

```{r ran.wknn.err, cache=TRUE, messages=FALSE}
ran.wknn.err = materr(ran.wknn.pre)
```

```{r ran.wknn.err.plot} 
ran.wknn.plot = melt(setDT(as.data.frame(ran.wknn.err), keep.rownames = TRUE), 'rn')
ggplot(aes(x=variable, y=value, group=rn, color = rn), 
       data = subset(ran.wknn.plot , rn %in% c('mae'))[-1,]) +
  geom_line() +
  ggtitle('Mean Absolute Error for WKnn') +
  xlab('DBA') + ylab('MAE')

ggplot(aes(x=variable, y=value, group=rn, color = rn), 
       data = subset(ran.wknn.plot , rn %in% c('mape'))[-1,]) +
  geom_line() +
  ggtitle('Mean Absolute Percentage Error for Regression') +
  xlab('DBA') + ylab('MAPE')
```


### 2.4 Random Forest

We select the 20 most important variables by random forest. 

```{r ran.rf.caret, cache=TRUE} 
#20 most important variables by RF
set.seed(996)
ran.rtree.pred2 = test
ran.rtree.pred2[3:ncol(ran.rtree.pred2)] = NA

variablebox = data.frame(matrix(0,nrow(test)*20, ncol=20)) #to store the variables
s1 = Sys.time()
for (i in 3:(ncol(test)-20)){
  this.train = train[, c(1, 2, i:ncol(train))] #select true, dow, then looping on ROH_DBA
  this.train = this.train[, -nearZeroVar(this.train)]  #eliminate zero variance variables
  #feature selection (top 20)
  fs = randomForest(True~., data=this.train, importance=TRUE, ntree=100) %>%
    importance() %>% data.frame() %>%
    rownames_to_column() %>%
    arrange(., desc(`X.IncMSE`))
  
  imp.20 = fs[1:20,]$rowname
  variablebox[i-2,] = imp.20
  
  this.train = this.train[, c(1, which(colnames(this.train) %in% imp.20))]
  this.test = test[, c(1, 2, i:ncol(test))] #although this test includes everything, later it only uses top20 features
  train.control=trainControl(method='repeatedcv', number=10, repeats = 3)
  r.tree = train(True~., data = this.train, method='rf', 
                 trControl=train.control, tuneLength = 10
                 #,tunegrid=expand.grid(.mtry=sqrt(ncol(this.train)))
                 )
  ran.rtree.pred2[, i] = predict(r.tree, this.test, type = 'raw')
  print(Sys.time()-s1)
}
```

```{r ran.rtree.err}
ran.rtree.err = materr(ran.rtree.pred2[,-c(1,2)])
```

```{r ran.rtree.err.plot}
ran.rtree.plot = melt(setDT(as.data.frame(ran.rtree.err), keep.rownames = TRUE), 'rn')
ggplot(aes(x=variable, y=value, group=rn, color = rn), 
       data = subset(ran.rtree.plot , rn %in% c('mae'))[-1,]) +
  geom_line() +
  ggtitle('Mean Absolute Error for rtree') +
  xlab('DBA') + ylab('MAE')

ggplot(aes(x=variable, y=value, group=rn, color = rn), 
       data = subset(ran.rtree.plot , rn %in% c('mape'))[-1,]) +
  geom_line() +
  ggtitle('Mean Absolute Percentage Error for Regression') +
  xlab('DBA') + ylab('MAPE')
```


