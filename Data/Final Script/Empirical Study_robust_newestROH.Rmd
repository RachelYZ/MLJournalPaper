---
title: "Pick-up method + machine learning: a proved efficient approach to forecast hotel demand (Robust test for newest ROH) "
author: "Rachel Zhang"
date: "06/18/2020"
output:
  pdf_document: default
  html_document: default
classoption: 
  - landscape
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE, 
                      out.height = "\\textheight",  out.width = "\\textwidth")
library(kableExtra)
library(tidyverse)
library(dplyr)
library(reshape2)
library(colorspace)
library(ggplot2)
library(texreg)
library(caret)
library(rpart)
library(e1071)
library(forecast)
library(neuralnet)
library(kknn)
library(dummies)
options(digits = 3)
setwd("~/Documents/Research/MLJournalPaper/Data")
```

```{r functions, include=FALSE}
convert_date = function(x){ #to convert date
  require(dplyr)
  new = as.Date(x, '%y.%m.%d') %>%
     format(., '20%y-%m-%d') %>%
    as.Date()
  return (new)
}

materr = function(data){  
  require(dplyr)
  m = data 
  for (j in 2:ncol(data)){
    m[,j] = (data[,j] - data[,1]) #calculating errors for each forecast
  }
  ME = colMeans(m)
  MAE = colMeans(abs(m))
  SD = apply(m,2,sd)
  p = data
  for (i in 2:ncol(p)){
    p[, i] = m[,i] / data[,1]
  }
  MPE = colMeans(p)
  MAPE = colMeans(abs(p))
  return(rbind(ME,MAE,MPE,MAPE,SD))
} 
```

# Data

## Import Dataset and Cross-Validation 

```{r loaddata}
dt <- read.csv("Arr.csv") %>% 
  data.frame()%>%
  select(Arrival.Date, Booking.Window, Quantity) %>%
  mutate_at(., vars(Arrival.Date),  funs(as.Date(., "%m/%d/%Y"))) %>%
  group_by(Arrival.Date, Booking.Window) %>%
  mutate(Quan = sum(Quantity)) %>%
  dplyr::arrange(., Arrival.Date, Booking.Window, Quan) 
dt = dt[-(1:2),-3] 
dt$Arrival.Date = convert_date(dt$Arrival.Date)
#---same----# 

wide = dcast(dt, Arrival.Date ~ Booking.Window, value.var='Quan') %>%
  data.frame() %>%
  arrange(., Arrival.Date)
for (i in (ncol(wide)-1):2){
  wide[i] = wide[i] + wide[i+1] 
}

agg = c(1, 2, 3, 4, 5, 6, 7, 14, 21, 30, 60, 90)
wide = wide %>%
  mutate(DOW = weekdays(Arrival.Date)) %>%
  remove_rownames %>% column_to_rownames('Arrival.Date') %>%
  select(X0, DOW, paste0("X",agg)) 
colnames(wide) = c('ROH0', 'DOW', paste0('ROH',agg))
wide$DOW <- ordered(wide$DOW, levels=c("Sunday","Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
```

```{r plotseries, fig.width=8, fig.height=4}
ggplot(wide, aes(x=as.Date(unlist(rownames(wide))), y=ROH0)) +
  geom_line(color='dimgray') +
  theme_minimal()+
  xlab("Stay Date") + ylab('Final Arrivals') +
  theme(plot.caption = element_text(hjust = 0), text = element_text(size=10)) 
```

This robust analysis is conducted under `R version 3.6.0` with `set.seed(123)`. 

We randomly selected 80% of the records as the training dataset to tune models, and the rest 20% records are used for model performance test. Here is a peek of the training set: 

```{r cv}
set.seed(123)
tr_ind = sample(nrow(wide), 0.8*nrow(wide))
train = wide[tr_ind, ]
test = wide[-tr_ind, ]
#-----same, data same-----
kable(train[1:10, ], 'latex', caption = 'Training Set Overview',
        longtable = F, booktabs = T) %>%
  kable_styling(latex_options = c("striped", "hold_position", "scale_down" ,"repeat_header"))
```

## Modeling
### Additive Pick-up
```{r apk}
#calculating additive pick up
apk = train    
for (j in ncol(apk):3){
  apk[,j] = apk[,1] - apk[,j]
}
apk = apk %>%
  group_by(DOW) %>%
  summarise_at(.vars=names(.)[3:ncol(apk)], .funs='mean')

apk.pre = test
apk.pre[, 3:ncol(apk.pre)] = NA

s1 = Sys.time()
for (i in 1:(nrow(apk.pre))){
  m = match(apk.pre[i,2], apk$DOW)
  apk.pre[i, 3:ncol(apk.pre)] = test[i,3:ncol(apk.pre)] + apk[m,2:ncol(apk)]
}
time.apk = Sys.time() - s1
apk.err = t(materr(apk.pre[,-2])) %>% data.frame()  
#same results#

kable(apk, 'latex', caption = 'Additive Pick Ups',
        longtable = F, booktabs = T) %>%
  kable_styling(latex_options = c("striped", "hold_position", "scale_down" ,"repeat_header")) %>%
  footnote(general = 'The pick-ups are calculated by taking the average of additive increments between current day and a future date by day of week.')
```

### Multiplicative Pick-up
```{r mpk}
mpk = train    
for (j in ncol(mpk):3){
  mpk[,j] = mpk[,j] / mpk[,1] 
}
mpk = mpk %>%
  group_by(DOW) %>%
  summarise_at(.vars=names(.)[3:ncol(mpk)], .funs='mean')

mpk.pre = test
mpk.pre[, 3:ncol(mpk.pre)] = NA

s1 = Sys.time()
for (i in 1:(nrow(mpk.pre))){
  m = match(mpk.pre[i,2], mpk$DOW)
  mpk.pre[i,3:ncol(mpk.pre)] = 
    test[i,3:ncol(mpk.pre)] / mpk[m, 2:ncol(mpk)]
}
time.mpk = Sys.time() - s1

mpk.err = t(materr(mpk.pre[,-2])) %>% data.frame() 
kable(mpk, 'latex', caption = 'Multiplicative Pick Ups',
        longtable = F, booktabs = T) %>%
  kable_styling(latex_options = c("striped", "hold_position", "scale_down" ,"repeat_header")) %>%
  footnote(general = 'The pick-ups are calculated by taking the average of ratio increments between current day and a future date by day of week.')
```

### Regression 

The regression model uses the nearest ROH and the DOW of the target day. 

```{r reg, results="asis"}
reg.pred = test
reg.pred[,3:ncol(reg.pred)] = NA
reg = vector(mode='list')
train$DOW = factor(train$DOW , ordered = FALSE )
s1 = Sys.time()
for (i in agg){ 
  this.predictor = paste0(paste0('ROH',agg[which(agg==i)]), collapse='+')
  lm.formula = paste('ROH0', paste0('DOW+', this.predictor), sep = '~')
  reg[[this.predictor]] = lm(lm.formula, data = train)
  reg.pred[, which(names(reg.pred)==paste0('ROH',i))]=predict(reg[[this.predictor]], test)
}
time.reg = Sys.time() - s1
reg.err = t(materr(reg.pred[,-2])) %>% data.frame() 
```

\newpage

```{r echo=FALSE, results = "asis"}
#reg[[1]]
texreg(list(reg[[1]], reg[[2]], reg[[3]], reg[[4]], reg[[5]], reg[[6]], reg[[7]], reg[[8]],reg[[9]],reg[[10]],reg[[11]],reg[[12]]))
#regresultshtml = tth(regresults)
```

 
### Neural Network

```{r nn2}
set.seed(123)
nn.train.scaled=scale(train[,-2]) #standardization
nn.test.scaled= scale(test[,-2])
dow.tr = dummy(train$DOW, sep='.')
dow.te = dummy(test$DOW, sep='.')
nn.train.scaled = data.frame(cbind(dow.tr, nn.train.scaled))
nn.test.scaled = data.frame(cbind(dow.te, nn.test.scaled))
nn.pre2 = nn.test.scaled %>% data.frame()
nn.pre2[, 9:ncol(nn.pre2)] = NA
nn.model = vector(mode='list')
#nn.err2 = read.csv('nn.err2.csv')
```

When building model, the number of hidden units is set as `3`. The dataset is scaled and DOW is converted to dummy variables. 

```{r}
s1=Sys.time()
for (i in (agg)){
  colind = which(names(as.data.frame(nn.train.scaled))==paste0('ROH',i))
  this.train = nn.train.scaled[, c(1:8, colind)]
  nn.model[[colind-8]] = neuralnet(ROH0~., this.train, linear.output=T, hidden=3,
                       stepmax = 1e+06)
  this.test = nn.test.scaled[, c(1:8, colind:ncol(nn.test.scaled))] %>% as.data.frame()
  nn.pre2[, colind] = predict(nn.model[[colind-8]], this.test)
}
time.nn = Sys.time() - s1

nn.pre2 = nn.pre2[, -(1:7)]
nn.pre2.unscaled = nn.pre2
nn.pre2.unscaled[,] = NA
for (i in 1:nrow(nn.pre2)){
  nn.pre2.unscaled[i,] = nn.pre2[i,]* (apply(test[,-2], 2, sd)) + (colMeans(test[,-2]))
}
nn.err2 = materr(nn.pre2.unscaled) %>% t() %>%
  as.data.frame()
```

Taking ROH=5 as the example, this plot provides a straightforward visualization of the relevant neural network. 

```{r plotnn, fig.width=20, fig.height=9}
png(file='nn5plot.png', width=10, height=8, res=300, bg='transparent')
plot(nn.model[[colind-8]], 
     radius = 0.1, arrow.length=0.15, information=FALSE, fontsize = 19,
     col.intercept = "dimgray", dimension=8)
#print(nn5plot)
dev.off()
#ggsave('nn5plot.png',dpi=300, width=10, height=8)
```


### K-Nearest Neighbor
```{r knn}
set.seed(123)
k.train = cbind(dow.tr, train[,-2])
k.test = cbind(dow.te, test[,-2])
knn.pre2 = test[,-2]
knn.pre2[,2:ncol(knn.pre2)]=NA
opk= rep(NA, length(agg))
s1=Sys.time()
for (i in agg){
  colind = which(names(k.train)==paste0('ROH',i))
  this.train = k.train[, c(1:8, colind)]
  train.control = trainControl(method='repeatedcv', number=10, preProcOptions = list(thresh=0.8)) #for PCA
  k = train(ROH0~., method='knn', tuneLength = 5,
            trControl=train.control, preProcess=c('scale','center','pca'),
            data=this.train)
  opk[colind-8] = k$bestTune[[1]]
  this.test = k.test
  knn.pre2[,colind-7] = predict(k, this.test)
}
time.knn = Sys.time() - s1
knn.err2 = materr(knn.pre2) %>% t() %>%
  as.data.frame()
opk
```

```{r wknn2}
set.seed(123)
wknn.pre2 = test[,-2]
wknn.pre2[, 2:ncol(wknn.pre2)] = NA
opk2 = list()
s1=Sys.time()
for (i in agg){
  # 2s per loop
  colind = which(names(k.train)==paste0('ROH',i))
  this.train = k.train[, c(1:8, colind)]
  train.control = trainControl(method='repeatedcv', number=10, preProcOptions = list(thresh=0.8))
  wk = train.kknn(ROH0~., data = this.train, kmax = 20,
                  kernel =  c("rectangular", "triangular", "epanechnikov", "gaussian", "rank", "optimal"))
  opk2[[colind-8]] = wk$best.parameters
#  kknn <- kknn(ROH0~., k=opk2[i-1], scale = ROH0, distance=2,
#               train = this.train, test = this.test,
#               kernel = paste0(toString(wk$best.parameters[[1]])))
  wknn.pre2[, colind-7] = predict(wk, k.test)
}
time.wknn = Sys.time() - s1
wknn.err2 = materr(wknn.pre2)%>% t() %>%
  as.data.frame()
opk
#same#
```

### Tree
```{r decision_tree2}
set.seed(123)
dtree.pre2 = test
dtree.pre2[, 3:ncol(dtree.pre2)] = NA
s1 = Sys.time()
for (i in agg){
  colind=which(names(train)==paste0('ROH',i))
  this.train = train[, c(1, 2, colind)]
  tree.model = rpart(ROH0~., data=this.train, method='anova')
  dtree.pre2[, colind] = predict(tree.model, test)
}
time.dtree = Sys.time() - s1
dtree.err2 = materr(dtree.pre2[,-2]) %>% t() %>%
  as.data.frame()
#same
```

```{r dtreeplot, fig.width=14, fig.height=10}
library(rpart.plot)
rpart.plot(tree.model, box.palette = 'Grays', nn=TRUE, extra = 1,
           cex=2)
```

```{r rf2}
set.seed(123)
rf.pre2 = test
rf.pre2[, 3:ncol(test)] = NA


mtry.rf = rep(0, length(agg)+1)
s1 = Sys.time()
for (i in agg){
  colind=which(names(train)==paste0('ROH',i))
  this.train = train[, c(1, 2, colind)]
  train.control=trainControl(method='repeatedcv', number=10)
  r.tree = train(ROH0~., data = this.train, method='rf',
               trControl=train.control, metric = 'RMSE')
  mtry.rf[colind-2] = r.tree$bestTune
  rf.pre2[, colind] = predict(r.tree, test, type = 'raw')
}
time.rf = Sys.time() - s1
#write_csv(rf.pre2, 'rf.pre2.csv')
#rf.pre2 =  read.csv("rf.pre2.csv")
rf.err2 = materr(rf.pre2[,-2])%>% t() %>%
  as.data.frame()
```

### Support Vector Regression

After some mannual cross validation, we choose the `radial` kernel for this empirical study, then test different `gamma` values for the model. Here shows the selected $\gamma$ values. Usually lower $\gamma$ indicates more linear boundary. 

```{r svm2}
set.seed(123)
svm.pre2 = test
svm.pre2[, 3:ncol(svm.pre2)] = NA
gamma = list()

s1 = Sys.time()
for (i in agg){
  colind=which(names(train)==paste0('ROH',i))
  this.train = train[, c(1, 2, colind)]
  svm.model = tune.svm(ROH0~., data=this.train, kernel="radial",
                 gamma=c(0.1,2^(-(0:5))))    
  gamma[[colind-2]] = svm.model$best.parameters[1,1]#  svm.tune = tune(svm, ROH0~., data = this.train, 
 #                 kernel = 'polynomial', ranges = list(degree = 1:5, coef0 = c(0.5,1,2)))
  svm.pre2[, colind] = predict(svm.model$best.model, test)
}
svm.err2 = materr(svm.pre2[,-2]) %>% t() %>%
  as.data.frame()
time.svm = Sys.time() - s1
gamma
# #select among linear, 
# svm.linear = tune.svm(ROH0~., data=this.train, kernel="linear", cost=c(0.001, 0.01, 0.1, 1,5,10))
# svm.linear$best.performance #5.38 - 9.66 - 18.5 - 143 
# 
# svm.poly = tune.svm(ROH0~., data=this.train, kernel="polynomial", 
#                     degree=c(2,3,4,5), coef0=c(0.1,0.5,1,2,3,4))
# svm.poly$best.performance  #5.28 - 9.55 - 17.3 - 149
# 
# svm.radial = tune.svm(ROH0~., data=this.train, kernel="radial",
# gamma=c(0.1,0.5,1,2,3,4))
# svm.radial$best.performance  #19.2 - 22.4 - 150
# 
# svm.sigmoid = tune.svm(ROH0~., data=this.train, kernel="sigmoid",
# gamma=c(0.1,0.5,1,2,3,4), coef0=c(0.1,0.5,1,2,3,4))
# svm.sigmoid$best.performance  #very bad - 23.9 - 158
```


## Results
```{r results_ME2}
ME_ALL = cbind(apk = apk.err[-1,1], mpk=mpk.err[-1,1],
              reg = reg.err[-1,1], nn = nn.err2[-1,1], knn = knn.err2[-1,1],
              wknn = wknn.err2[-1,1], dtree = dtree.err2[-1,1],
              rf = rf.err2[-1,1], svm = svm.err2[-1,1]) %>%
  as.data.frame()
rownames(ME_ALL) = c(paste0('DBA',agg))

# MEmelt = melt(as.matrix(ME_ALL), varnames=c('DBA', 'Model'))
# MEmelt$DBA = factor(MEmelt$DBA,
#                   levels = c(paste0('DBA', agg)))
# MEmelt$Model = factor(MEmelt$Model,
#                       levels=c('apk','mpk','reg','nn','knn','wknn', 'dtree', 'rf','svm'))
#
# ggplot(MEmelt, aes(x=DBA, y=value, group=Model, color=Model)) +
#   geom_line(aes(color=Model), size=1)+
#   geom_point(aes(color=Model), size=1)+
#   scale_color_brewer(palette="Paired")+
#   xlab('') + ylab('Mean Errors') + ylim(-10,10) +
#   theme_minimal()+
#   theme(axis.text.x = element_text(angle=45, vjust=0.5))

ME_ALL[13,] = colMeans(ME_ALL)
kable(ME_ALL, 'latex', caption = 'Mean Errors',
      booktabs = T) %>%
  kable_styling(latex_options = c("striped", "repeat_header", 'hold_position'))
```

```{r results_MAE2}
MAE_ALL = cbind(apk = apk.err[-1,2], mpk=mpk.err[-1,2],
              reg = reg.err[-1,2],  nn = nn.err2[-1,2], knn = knn.err2[-1,2],
              wknn = wknn.err2[-1,2], dtree = dtree.err2[-1,2],
              rf = rf.err2[-1,2], svm = svm.err2[-1,2]) %>%
  as.data.frame()
rownames(MAE_ALL) = c(paste0('DBA',agg))
#
# MAEmelt = melt(as.matrix(MAE_ALL), varnames=c('DBA', 'Model'))
# MAEmelt$DBA = factor(MAEmelt$DBA,
#               levels = c(paste0('DBA', agg)))
# MAEmelt$Model = factor(MAEmelt$Model,
#               levels=c('apk','mpk','reg','nn','knn','wknn', 'dtree', 'rf','svm'))
# ggplot(MAEmelt, aes(x=DBA, y=value, group=Model, color=Model)) +
#   geom_line(aes(color=Model), size=1)+
#   geom_point(aes(color=Model), size=1)+
#   scale_color_brewer(palette="Paired")+
#   xlab('') + ylab('Mean Absolute Errors') +
#   theme_minimal()+ ylim(0,15)+
#   theme(axis.text.x = element_text(angle=45, vjust=0.5))

MAE_ALL[13,] = colMeans(MAE_ALL)
kable(MAE_ALL, 'latex', caption = 'Mean Absolute Errors',
      booktabs = T) %>%
  kable_styling(latex_options = c("striped", "repeat_header", 'hold_position'))
```

```{r results_SDE2}
SDE_ALL = cbind(apk = apk.err[-1,5], mpk=mpk.err[-1,5],
              reg = reg.err[-1,5],  nn = nn.err2[-1,5], knn = knn.err2[-1,5],
              wknn = wknn.err2[-1,5], dtree = dtree.err2[-1,5],
              rf = rf.err2[-1,5], svm = svm.err2[-1,5] ) %>%
  as.data.frame()
rownames(SDE_ALL) = c(paste0('DBA',agg))
#
# SDEmelt = melt(as.matrix(SDE_ALL), varnames=c('DBA', 'Model'))
# SDEmelt$DBA = factor(SDEmelt$DBA,
#               levels = c(paste0('DBA', agg)))
# SDEmelt$Model = factor(SDEmelt$Model,
#               levels=c('apk','mpk','reg','nn','knn','wknn', 'dtree', 'rf','svm'))
# ggplot(SDEmelt, aes(x=DBA, y=value, group=Model, color=Model)) +
#   geom_line(aes(color=Model), size=1)+
#   geom_point(aes(color=Model), size=1)+
#   scale_color_brewer(palette="Paired")+
#   xlab('') + ylab('Standard Deviations') +
#   theme_minimal()+ ylim(0,15)+
#   theme(axis.text.x = element_text(angle=45, vjust=0.5))

SDE_ALL[13,] = colMeans(SDE_ALL)
kable(SDE_ALL, 'latex', caption = 'Standard Deviation Errors',
      booktabs = T) %>%
  kable_styling(latex_options = c("striped", "repeat_header", 'hold_position'))
#write.csv(cbind(MEALL, MAEALL, MPEALL, MAPEALL, SD), "ran_e1_bycutoff.csv")
```

```{r results_MPE2}
MPE_ALL = cbind(apk = apk.err[-1,3], mpk=mpk.err[-1,3],
              reg = reg.err[-1,3],  nn = nn.err2[-1,3], knn = knn.err2[-1,3],
              wknn = wknn.err2[-1,3], dtree = dtree.err2[-1,3],
              rf = rf.err2[-1,3], svm = svm.err2[-1,3]) %>%
  as.data.frame()
rownames(MPE_ALL) = c(paste0('DBA',agg))
#
# MPEmelt = melt(as.matrix(MPE_ALL), varnames=c('DBA', 'Model'))
# MPEmelt$DBA = factor(MPEmelt$DBA,
#               levels = c(paste0('DBA', agg)))
# MPEmelt$Model = factor(MPEmelt$Model,
#               levels=c('apk','mpk','reg','nn','knn','wknn', 'dtree', 'rf','svm'))
# ggplot(MPEmelt, aes(x=DBA, y=value, group=Model, color=Model)) +
#   geom_line(aes(color=Model), size=1)+
#   geom_point(aes(color=Model), size=1)+
#   scale_color_brewer(palette="Paired")+
#   xlab('') + ylab('Mean Percentage Errors') +
#   theme_minimal()+
#   theme(axis.text.x = element_text(angle=45, vjust=0.5))

MPE_ALL[13,] = colMeans(MPE_ALL)
kable(MPE_ALL, 'latex', caption = 'Mean Percentage Errors',
      booktabs = T) %>%
  kable_styling(latex_options = c("striped", "repeat_header", 'hold_position'))
```

```{r results_MAPE2}
MAPE_ALL = cbind(apk = apk.err[-1,4], mpk=mpk.err[-1,4],
              reg = reg.err[-1,4],  nn = nn.err2[-1,4], knn = knn.err2[-1,4],
              wknn = wknn.err2[-1,4], dtree = dtree.err2[-1,4],
              rf = rf.err2[-1,4], svm = svm.err2[-1,4]) %>%
  as.data.frame()
rownames(MAPE_ALL) = c(paste0('DBA',agg))

# MAPEmelt = melt(as.matrix(MAPE_ALL), varnames=c('DBA', 'Model'))
# MAPEmelt$DBA = factor(MAPEmelt$DBA,
#               levels = c(paste0('DBA', agg)))
# MAPEmelt$Model = factor(MAPEmelt$Model,
#               levels=c('apk','mpk','reg','nn','knn','wknn', 'dtree', 'rf','svm'))
# ggplot(MAPEmelt, aes(x=DBA, y=value, group=Model, color=Model)) +
#   geom_line(aes(color=Model), size=1)+
#   geom_point(aes(color=Model), size=1)+
#   scale_color_brewer(palette="Paired")+
#   xlab('') + ylab('Mean Absolute Percentage Errors') +
#   theme_minimal()+
#   theme(axis.text.x = element_text(angle=45, vjust=0.5))

MAPE_ALL[13,] = colMeans(MAPE_ALL)
kable(MAPE_ALL, row.names = NA, 'latex', caption = 'MAPE',
      booktabs = T) %>%
  kable_styling(latex_options = c("striped", "repeat_header",  'hold_position'))
```

```{r results_allavg}
time = c(time.apk, time.mpk, time.reg, time.nn, time.knn, time.wknn, 
         time.dtree, time.rf, time.svm)
ALL = rbind(ME = ME_ALL[13,], `MAE` = MAE_ALL[13,], `SDE` = SDE_ALL[13,], 
            `MPE` = MPE_ALL[13,],
            `MAPE`=MAPE_ALL[13,], 
            `Time` = time) %>% t()
model_list = c('Additive Pickup','Multiplicative Pickup', 'Regression',
               'Neural Network', 'K-Nearest Neighbor', 'Weighted K-Nearest Neighbor', 
               'Decision Tree', 'Random Forest', 'Support Vector Machine')
rownames(ALL) = model_list
kable(ALL, 'latex', caption = 'Model Performances',
      booktabs = T) %>%
  kable_styling(latex_options = c("striped", "repeat_header", 'hold_position')) %>%
  footnote(number = c('Mean Error', 'Mean Absolute Error', 'Standard Deviation Error',
                      'Mean Percentage Error', 'Mean Absolute Percentage Error', 
                      'Time is calculated in seconds'))

#ALL = read.csv("~/Documents/Research/MLJournalPaper/Data/Final Script/results_newestROH.csv")
write.csv(ALL, file='robust_results_newestROH.csv')
```



 